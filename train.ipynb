{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import picamera\n",
    "import picamera.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'dataset'\n",
    "TRAINING_DATA = []  # Example array to be trained\n",
    "TRAINING_LABELS = []  # Label array\n",
    "NUM_CLASSES = 0\n",
    "count = [0, 0]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "DENSE_UNITS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    print(\"get image called\")\n",
    "    global NUM_CLASSES\n",
    "    image_path = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    classes = [p.split('/')[1] for p in image_path]\n",
    "    NUM_CLASSES = len(classes)\n",
    "\n",
    "    print(NUM_CLASSES)\n",
    "    print(classes)\n",
    "    for cls in classes:\n",
    "        pp = path + \"/\" + cls\n",
    "        print(\"class={}\".format(pp))\n",
    "        image_path = [ os.path.join(pp, f) for f in os.listdir(pp) if f.endswith('.jpg') ]\n",
    "#         print(image_path)\n",
    "        print(len(image_path))\n",
    "        for img in image_path:\n",
    "#             print(\"path={}\".format(img))\n",
    "            add_example(prepare_frame(cv2.imread(img)), int(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_example(example, label):  # add examples to training data set\n",
    "    encoded_y = keras.utils.np_utils.to_categorical(label, num_classes=NUM_CLASSES)  # make one-hot\n",
    "    TRAINING_LABELS.append(encoded_y)\n",
    "    TRAINING_DATA.append(example[0])\n",
    "#     print('> add example for label %d' % label)\n",
    "    count[label] += 1\n",
    "\n",
    "\n",
    "def prepare_frame(frame):\n",
    "    img = Image.fromarray(frame, 'RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array_extended = np.expand_dims(img_array, axis=0).astype('float32')\n",
    "    processed = keras.applications.mobilenet.preprocess_input(img_array_extended)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get image called\n",
      "2\n",
      "['1', '0']\n",
      "class=dataset/1\n",
      "5\n",
      "class=dataset/0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "get_image(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,GlobalAveragePooling2D,Dropout,SeparableConv2D,BatchNormalization, Activation, Dense\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def load_model():\n",
    "    mobilenet = keras.applications.mobilenet.MobileNet()\n",
    "#     base_model = mobilenet\n",
    "#     x=base_model.output\n",
    "#     # Add some new Fully connected layers to \n",
    "#     x=GlobalAveragePooling2D()(x)\n",
    "#     x=Dense(1024,activation='relu')(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "#     x=Dense(512,activation='relu')(x) \n",
    "#     x = Dropout(0.25)(x)\n",
    "#     preds=Dense(2, activation='softmax')(x) #final layer with softmax activation\n",
    "#     model=Model(inputs=base_model.input,outputs=preds)    \n",
    "    flatten = Flatten(input_shape=(7, 7, 1024))(mobilenet.get_layer('conv_pw_13_relu').output)\n",
    "    fc1 = Dense(DENSE_UNITS, activation='relu')(flatten)\n",
    "    fc2 = Dense(NUM_CLASSES)(flatten)\n",
    "    output = Activation('softmax')(fc2)\n",
    "    model = Model(mobilenet.input, output)\n",
    "#     # make all layers untrainable by freezing weights (except for last two layers)\n",
    "#     for l, layer in enumerate(model.layers[:-3]):\n",
    "#         layer.trainable = False\n",
    "#     # ensure the last layer is trainable/not frozen\n",
    "#     for l, layer in enumerate(model.layers[-3:]):\n",
    "#         layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "learning_rate = 0.0005\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "model = load_model()\n",
    "for i,layer in enumerate(model.layers):\n",
    "    print(\"{}: {}\".format(i,layer))\n",
    "# model.summary()\n",
    "\n",
    "for layer in model.layers[:87]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[87:]:\n",
    "    layer.trainable=True\n",
    "opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)    \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(TRAINING_DATA), np.array(TRAINING_LABELS), epochs=epochs, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # with picamera.PiCamera() as camera:\n",
    "# #     camera.resolution = (1024, 768)\n",
    "# #     camera.framerate = 24    \n",
    "# #     with picamera.array.PiRGBArray(camera) as output:      \n",
    "# #         camera.capture(output, 'rgb')\n",
    "# #         print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "# #         frame = output.array\n",
    "# #         jpeg_image = bgr8_to_jpeg(frame)\n",
    "# #         image_widget.value = jpeg_image\n",
    "        \n",
    "# with picamera.PiCamera() as camera:\n",
    "#     camera.resolution = (480, 320)\n",
    "#     camera.framerate = 24        \n",
    "#     with picamera.array.PiRGBArray(camera) as output:\n",
    "#         while True:        \n",
    "#             camera.capture(output, 'rgb')\n",
    "#             print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "#             frame = output.array\n",
    "#             processed_image = prepare_frame(frame)  # prepare frame\n",
    "#             prediction = model.predict(processed_image)\n",
    "#             result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "#             print('predict class= %d' % result)\n",
    "#             output.truncate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "text = widgets.Text(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# from __future__ import print_function\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "from picamera.array import PiRGBArray\n",
    "from picamera import PiCamera\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "from imutils.video import FPS\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "#camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# initialize the camera and stream\n",
    "camera = PiCamera()\n",
    "camera.resolution = (320, 240)\n",
    "camera.framerate = 32\n",
    "camera.start_preview()\n",
    "rawCapture = PiRGBArray(camera, size=(320, 240))\n",
    "stream = camera.capture_continuous(rawCapture, format=\"bgr\",\n",
    "use_video_port=True)\n",
    "\n",
    "image_widget = widgets.Image()\n",
    "display(text)\n",
    "display(image_widget)\n",
    "\n",
    "\n",
    "for (i, f) in enumerate(stream):\n",
    "    frame = f.array\n",
    "    image_widget.value =  bgr8_to_jpeg(frame)\n",
    "    processed_image = prepare_frame(frame)  # prepare frame\n",
    "    t1 = datetime.datetime.now()\n",
    "    prediction = model.predict(processed_image)\n",
    "    t2 = datetime.datetime.now()\n",
    "    result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "    text.value = \"class=\" + str(result)\n",
    "    rawCapture.truncate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
