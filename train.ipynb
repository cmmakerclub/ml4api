{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import picamera\n",
    "import picamera.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'dataset'\n",
    "TRAINING_DATA = []  # Example array to be trained\n",
    "TRAINING_LABELS = []  # Label array\n",
    "NUM_CLASSES = 0\n",
    "count = [0, 0]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 10\n",
    "DENSE_UNITS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "#image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "#camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class0 = 'dataset/0'\n",
    "# class1 = 'dataset/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    print(\"get image called\")\n",
    "    global NUM_CLASSES\n",
    "    image_path = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    classes = [p.split('/')[1] for p in image_path]\n",
    "    NUM_CLASSES = len(classes)\n",
    "\n",
    "    print(NUM_CLASSES)\n",
    "    print(classes)\n",
    "#     classes = classes[1:]\n",
    "#     print(classes)\n",
    "    for cls in classes:\n",
    "        pp = path + \"/\" + cls\n",
    "        print(\"class={}\".format(pp))\n",
    "        image_path = [ os.path.join(pp, f) for f in os.listdir(pp) if f.endswith('.jpg') ]\n",
    "        print(image_path)\n",
    "        print(len(image_path))\n",
    "        for img in image_path:\n",
    "            print(\"path={}\".format(img))\n",
    "            add_example(prepare_frame(cv2.imread(img)), int(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_example(example, label):  # add examples to training data set\n",
    "    encoded_y = keras.utils.np_utils.to_categorical(label, num_classes=NUM_CLASSES)  # make one-hot\n",
    "    TRAINING_LABELS.append(encoded_y)\n",
    "    TRAINING_DATA.append(example[0])\n",
    "    print('> add example for label %d' % label)\n",
    "    count[label] += 1\n",
    "\n",
    "\n",
    "def prepare_frame(frame):\n",
    "    img = Image.fromarray(frame, 'RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array_extended = np.expand_dims(img_array, axis=0).astype('float32')\n",
    "    processed = keras.applications.mobilenet.preprocess_input(img_array_extended)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get image called\n",
      "2\n",
      "['1', '0']\n",
      "class=dataset/1\n",
      "['dataset/1/1574682713.6998224.jpg', 'dataset/1/1574682720.8157759.jpg', 'dataset/1/1574682733.5838559.jpg', 'dataset/1/1574682715.5508034.jpg', 'dataset/1/1574682718.302046.jpg', 'dataset/1/1574682723.8483438.jpg', 'dataset/1/1574682714.632949.jpg', 'dataset/1/1574682725.6288648.jpg', 'dataset/1/1574682717.436506.jpg', 'dataset/1/1574682712.8491585.jpg', 'dataset/1/1574682716.4929962.jpg', 'dataset/1/1574682719.899265.jpg', 'dataset/1/1574682728.2821305.jpg', 'dataset/1/1574682727.1334388.jpg', 'dataset/1/1574682721.750257.jpg', 'dataset/1/1574682729.1823766.jpg', 'dataset/1/1574682724.7404063.jpg', 'dataset/1/1574682732.626225.jpg', 'dataset/1/1574682722.6861837.jpg']\n",
      "19\n",
      "path=dataset/1/1574682713.6998224.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682720.8157759.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682733.5838559.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682715.5508034.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682718.302046.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682723.8483438.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682714.632949.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682725.6288648.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682717.436506.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682712.8491585.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682716.4929962.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682719.899265.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682728.2821305.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682727.1334388.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682721.750257.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682729.1823766.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682724.7404063.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682732.626225.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574682722.6861837.jpg\n",
      "> add example for label 1\n",
      "class=dataset/0\n",
      "['dataset/0/1574682691.7779808.jpg', 'dataset/0/1574682705.6002872.jpg', 'dataset/0/1574682700.01869.jpg', 'dataset/0/1574682703.7486966.jpg', 'dataset/0/1574682704.6647508.jpg', 'dataset/0/1574682710.0743155.jpg', 'dataset/0/1574682694.5835993.jpg', 'dataset/0/1574682696.4498155.jpg', 'dataset/0/1574682693.678818.jpg', 'dataset/0/1574682706.5319867.jpg', 'dataset/0/1574682695.518014.jpg', 'dataset/0/1574682700.9498136.jpg', 'dataset/0/1574682698.1571572.jpg', 'dataset/0/1574682699.0835674.jpg', 'dataset/0/1574682710.976966.jpg', 'dataset/0/1574682708.331746.jpg', 'dataset/0/1574682707.4693239.jpg', 'dataset/0/1574682711.9167626.jpg', 'dataset/0/1574682709.2108474.jpg', 'dataset/0/1574682692.7069812.jpg', 'dataset/0/1574682701.882748.jpg', 'dataset/0/1574682702.814981.jpg', 'dataset/0/1574682697.3007615.jpg']\n",
      "23\n",
      "path=dataset/0/1574682691.7779808.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682705.6002872.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682700.01869.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682703.7486966.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682704.6647508.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682710.0743155.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682694.5835993.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682696.4498155.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682693.678818.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682706.5319867.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682695.518014.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682700.9498136.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682698.1571572.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682699.0835674.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682710.976966.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682708.331746.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682707.4693239.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682711.9167626.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682709.2108474.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682692.7069812.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682701.882748.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682702.814981.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574682697.3007615.jpg\n",
      "> add example for label 0\n"
     ]
    }
   ],
   "source": [
    "get_image(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model():\n",
    "    mobilenet = keras.applications.mobilenet.MobileNet()\n",
    "    flatten = Flatten(input_shape=(7, 7, 1024))(mobilenet.get_layer('conv_pw_13_relu').output)\n",
    "    fc1 = Dense(DENSE_UNITS, activation='relu')(flatten)\n",
    "    fc2 = Dense(NUM_CLASSES)(flatten)\n",
    "    output = Activation('softmax')(fc2)\n",
    "    model = Model(mobilenet.input, output)\n",
    "    # make all layers untrainable by freezing weights (except for last two layers)\n",
    "    for l, layer in enumerate(model.layers[:-3]):\n",
    "        layer.trainable = False\n",
    "    # ensure the last layer is trainable/not frozen\n",
    "    for l, layer in enumerate(model.layers[-3:]):\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x549f98f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 31s 733ms/step - loss: 11.8761 - accuracy: 0.5952\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 28s 659ms/step - loss: 2.5003 - accuracy: 0.7619\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 28s 657ms/step - loss: 1.9948 - accuracy: 0.7857\n",
      "Epoch 4/10\n",
      " 8/42 [====>.........................] - ETA: 22s - loss: 0.0493 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(TRAINING_DATA), np.array(TRAINING_LABELS), epochs=EPOCHS, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with picamera.PiCamera() as camera:\n",
    "    with picamera.array.PiRGBArray(camera) as output:\n",
    "        camera.capture(output, 'rgb')\n",
    "        print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "        frame = output.array\n",
    "        processed_image = prepare_frame(frame)  # prepare frame\n",
    "        prediction = model.predict(processed_image)\n",
    "        result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "        print('predict class= %d' % result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
