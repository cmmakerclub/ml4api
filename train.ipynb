{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import picamera\n",
    "import picamera.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'dataset'\n",
    "TRAINING_DATA = []  # Example array to be trained\n",
    "TRAINING_LABELS = []  # Label array\n",
    "NUM_CLASSES = 0\n",
    "count = [0, 0]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "DENSE_UNITS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "#image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "#camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "#display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class0 = 'dataset/0'\n",
    "# class1 = 'dataset/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    print(\"get image called\")\n",
    "    global NUM_CLASSES\n",
    "    image_path = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    classes = [p.split('/')[1] for p in image_path]\n",
    "    NUM_CLASSES = len(classes)\n",
    "\n",
    "    print(NUM_CLASSES)\n",
    "    print(classes)\n",
    "#     classes = classes[1:]\n",
    "#     print(classes)\n",
    "    for cls in classes:\n",
    "        pp = path + \"/\" + cls\n",
    "        print(\"class={}\".format(pp))\n",
    "        image_path = [ os.path.join(pp, f) for f in os.listdir(pp) if f.endswith('.jpg') ]\n",
    "        print(image_path)\n",
    "        print(len(image_path))\n",
    "        for img in image_path:\n",
    "            print(\"path={}\".format(img))\n",
    "            add_example(prepare_frame(cv2.imread(img)), int(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_example(example, label):  # add examples to training data set\n",
    "    encoded_y = keras.utils.np_utils.to_categorical(label, num_classes=NUM_CLASSES)  # make one-hot\n",
    "    TRAINING_LABELS.append(encoded_y)\n",
    "    TRAINING_DATA.append(example[0])\n",
    "    print('> add example for label %d' % label)\n",
    "    count[label] += 1\n",
    "\n",
    "\n",
    "def prepare_frame(frame):\n",
    "    img = Image.fromarray(frame, 'RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array_extended = np.expand_dims(img_array, axis=0).astype('float32')\n",
    "    processed = keras.applications.mobilenet.preprocess_input(img_array_extended)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get image called\n",
      "2\n",
      "['1', '0']\n",
      "class=dataset/1\n",
      "['dataset/1/1574683111.1488442.jpg', 'dataset/1/1574683113.7779195.jpg', 'dataset/1/1574683114.8496027.jpg', 'dataset/1/1574683112.8299692.jpg', 'dataset/1/1574683115.7303994.jpg']\n",
      "5\n",
      "path=dataset/1/1574683111.1488442.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574683113.7779195.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574683114.8496027.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574683112.8299692.jpg\n",
      "> add example for label 1\n",
      "path=dataset/1/1574683115.7303994.jpg\n",
      "> add example for label 1\n",
      "class=dataset/0\n",
      "['dataset/0/1574683085.89735.jpg', 'dataset/0/1574683082.9215724.jpg', 'dataset/0/1574683084.4689193.jpg', 'dataset/0/1574683087.611756.jpg']\n",
      "4\n",
      "path=dataset/0/1574683085.89735.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574683082.9215724.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574683084.4689193.jpg\n",
      "> add example for label 0\n",
      "path=dataset/0/1574683087.611756.jpg\n",
      "> add example for label 0\n"
     ]
    }
   ],
   "source": [
    "get_image(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model():\n",
    "    mobilenet = keras.applications.mobilenet.MobileNet()\n",
    "    flatten = Flatten(input_shape=(7, 7, 1024))(mobilenet.get_layer('conv_pw_13_relu').output)\n",
    "    fc1 = Dense(DENSE_UNITS, activation='relu')(flatten)\n",
    "    fc2 = Dense(NUM_CLASSES)(flatten)\n",
    "    output = Activation('softmax')(fc2)\n",
    "    model = Model(mobilenet.input, output)\n",
    "    # make all layers untrainable by freezing weights (except for last two layers)\n",
    "    for l, layer in enumerate(model.layers[:-3]):\n",
    "        layer.trainable = False\n",
    "    # ensure the last layer is trainable/not frozen\n",
    "    for l, layer in enumerate(model.layers[-3:]):\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x55dde9b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 9s 997ms/step - loss: 1.2537 - accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 6s 656ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 6s 651ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 6s 656ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 6s 651ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x6f952110>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(TRAINING_DATA), np.array(TRAINING_LABELS), epochs=EPOCHS, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 1\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n",
      "predict class= 0\n",
      "Captured 1080x1920 image\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c76936646c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprocessed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prepare frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# imagenet_utils.decode_predictions(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict class= %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with picamera.PiCamera() as camera:\n",
    "    with picamera.array.PiRGBArray(camera) as output:\n",
    "        while True:        \n",
    "            camera.capture(output, 'rgb')\n",
    "            print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "            frame = output.array\n",
    "            processed_image = prepare_frame(frame)  # prepare frame\n",
    "            prediction = model.predict(processed_image)\n",
    "            result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "            print('predict class= %d' % result)\n",
    "            output.truncate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
