{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import picamera\n",
    "import picamera.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'dataset'\n",
    "TRAINING_DATA = []  # Example array to be trained\n",
    "TRAINING_LABELS = []  # Label array\n",
    "NUM_CLASSES = 0\n",
    "count = [0, 0]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 10\n",
    "DENSE_UNITS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    print(\"get image called\")\n",
    "    global NUM_CLASSES\n",
    "    image_path = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    classes = [p.split('/')[1] for p in image_path]\n",
    "    NUM_CLASSES = len(classes)\n",
    "\n",
    "    print(NUM_CLASSES)\n",
    "    print(classes)\n",
    "    for cls in classes:\n",
    "        pp = path + \"/\" + cls\n",
    "        print(\"class={}\".format(pp))\n",
    "        image_path = [ os.path.join(pp, f) for f in os.listdir(pp) if f.endswith('.jpg') ]\n",
    "#         print(image_path)\n",
    "        print(len(image_path))\n",
    "        for img in image_path:\n",
    "#             print(\"path={}\".format(img))\n",
    "            add_example(prepare_frame(cv2.imread(img)), int(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_example(example, label):  # add examples to training data set\n",
    "    encoded_y = keras.utils.np_utils.to_categorical(label, num_classes=NUM_CLASSES)  # make one-hot\n",
    "    TRAINING_LABELS.append(encoded_y)\n",
    "    TRAINING_DATA.append(example[0])\n",
    "#     print('> add example for label %d' % label)\n",
    "    count[label] += 1\n",
    "\n",
    "\n",
    "def prepare_frame(frame):\n",
    "    img = Image.fromarray(frame, 'RGB')\n",
    "    img = img.resize((224, 224))\n",
    "    img_array = np.array(img)\n",
    "    img_array_extended = np.expand_dims(img_array, axis=0).astype('float32')\n",
    "    processed = keras.applications.mobilenet.preprocess_input(img_array_extended)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get image called\n",
      "2\n",
      "['1', '0']\n",
      "class=dataset/1\n",
      "23\n",
      "class=dataset/0\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "get_image(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,GlobalAveragePooling2D,Dropout,SeparableConv2D,BatchNormalization, Activation, Dense\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def load_model():\n",
    "    mobilenet = keras.applications.mobilenet.MobileNet()\n",
    "#     base_model = mobilenet\n",
    "#     x=base_model.output\n",
    "#     # Add some new Fully connected layers to \n",
    "#     x=GlobalAveragePooling2D()(x)\n",
    "#     x=Dense(1024,activation='relu')(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "#     x=Dense(512,activation='relu')(x) \n",
    "#     x = Dropout(0.25)(x)\n",
    "#     preds=Dense(2, activation='softmax')(x) #final layer with softmax activation\n",
    "#     model=Model(inputs=base_model.input,outputs=preds)    \n",
    "    flatten = Flatten(input_shape=(7, 7, 1024))(mobilenet.get_layer('conv_pw_13_relu').output)\n",
    "    fc1 = Dense(DENSE_UNITS, activation='relu')(flatten)\n",
    "    fc2 = Dense(NUM_CLASSES)(flatten)\n",
    "    output = Activation('softmax')(fc2)\n",
    "    model = Model(mobilenet.input, output)\n",
    "#     # make all layers untrainable by freezing weights (except for last two layers)\n",
    "    for l, layer in enumerate(model.layers[:-3]):\n",
    "        layer.trainable = False\n",
    "    # ensure the last layer is trainable/not frozen\n",
    "    for l, layer in enumerate(model.layers[-3:]):\n",
    "        layer.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <keras.engine.input_layer.InputLayer object at 0x54f2efb0>\n",
      "1: <keras.layers.convolutional.ZeroPadding2D object at 0x5603a810>\n",
      "2: <keras.layers.convolutional.Conv2D object at 0x5609fb50>\n",
      "3: <keras.layers.normalization.BatchNormalization object at 0x5609fad0>\n",
      "4: <keras.layers.advanced_activations.ReLU object at 0x5609f6f0>\n",
      "5: <keras.layers.convolutional.DepthwiseConv2D object at 0x56055ab0>\n",
      "6: <keras.layers.normalization.BatchNormalization object at 0x56070ef0>\n",
      "7: <keras.layers.advanced_activations.ReLU object at 0x56070370>\n",
      "8: <keras.layers.convolutional.Conv2D object at 0x56020bf0>\n",
      "9: <keras.layers.normalization.BatchNormalization object at 0x55fcc9d0>\n",
      "10: <keras.layers.advanced_activations.ReLU object at 0x55fcccf0>\n",
      "11: <keras.layers.convolutional.ZeroPadding2D object at 0x55f7ff10>\n",
      "12: <keras.layers.convolutional.DepthwiseConv2D object at 0x55f7fab0>\n",
      "13: <keras.layers.normalization.BatchNormalization object at 0x55f602b0>\n",
      "14: <keras.layers.advanced_activations.ReLU object at 0x55f75c70>\n",
      "15: <keras.layers.convolutional.Conv2D object at 0x55f60530>\n",
      "16: <keras.layers.normalization.BatchNormalization object at 0x55ebcdf0>\n",
      "17: <keras.layers.advanced_activations.ReLU object at 0x55ebcb50>\n",
      "18: <keras.layers.convolutional.DepthwiseConv2D object at 0x55f0e770>\n",
      "19: <keras.layers.normalization.BatchNormalization object at 0x55eefaf0>\n",
      "20: <keras.layers.advanced_activations.ReLU object at 0x55ed1ef0>\n",
      "21: <keras.layers.convolutional.Conv2D object at 0x55e9cb90>\n",
      "22: <keras.layers.normalization.BatchNormalization object at 0x55e67550>\n",
      "23: <keras.layers.advanced_activations.ReLU object at 0x55e67350>\n",
      "24: <keras.layers.convolutional.ZeroPadding2D object at 0x55e17490>\n",
      "25: <keras.layers.convolutional.DepthwiseConv2D object at 0x55e174f0>\n",
      "26: <keras.layers.normalization.BatchNormalization object at 0x55df7b90>\n",
      "27: <keras.layers.advanced_activations.ReLU object at 0x55df7b10>\n",
      "28: <keras.layers.convolutional.Conv2D object at 0x55d91c10>\n",
      "29: <keras.layers.normalization.BatchNormalization object at 0x55da6e10>\n",
      "30: <keras.layers.advanced_activations.ReLU object at 0x55da60b0>\n",
      "31: <keras.layers.convolutional.DepthwiseConv2D object at 0x55d58530>\n",
      "32: <keras.layers.normalization.BatchNormalization object at 0x55d06610>\n",
      "33: <keras.layers.advanced_activations.ReLU object at 0x55d06f70>\n",
      "34: <keras.layers.convolutional.Conv2D object at 0x55d37650>\n",
      "35: <keras.layers.normalization.BatchNormalization object at 0x55ce6f10>\n",
      "36: <keras.layers.advanced_activations.ReLU object at 0x55ccdad0>\n",
      "37: <keras.layers.convolutional.ZeroPadding2D object at 0x55c976d0>\n",
      "38: <keras.layers.convolutional.DepthwiseConv2D object at 0x55cafb30>\n",
      "39: <keras.layers.normalization.BatchNormalization object at 0x55c75a70>\n",
      "40: <keras.layers.advanced_activations.ReLU object at 0x55c0ce90>\n",
      "41: <keras.layers.convolutional.Conv2D object at 0x55c27eb0>\n",
      "42: <keras.layers.normalization.BatchNormalization object at 0x55bc0850>\n",
      "43: <keras.layers.advanced_activations.ReLU object at 0x55bd5ef0>\n",
      "44: <keras.layers.convolutional.DepthwiseConv2D object at 0x55becbd0>\n",
      "45: <keras.layers.normalization.BatchNormalization object at 0x55b9b850>\n",
      "46: <keras.layers.advanced_activations.ReLU object at 0x55b87bf0>\n",
      "47: <keras.layers.convolutional.Conv2D object at 0x55b50c10>\n",
      "48: <keras.layers.normalization.BatchNormalization object at 0x55aff9f0>\n",
      "49: <keras.layers.advanced_activations.ReLU object at 0x55affd10>\n",
      "50: <keras.layers.convolutional.DepthwiseConv2D object at 0x55b31f30>\n",
      "51: <keras.layers.normalization.BatchNormalization object at 0x55adfe30>\n",
      "52: <keras.layers.advanced_activations.ReLU object at 0x55adf6d0>\n",
      "53: <keras.layers.convolutional.Conv2D object at 0x55a8f570>\n",
      "54: <keras.layers.normalization.BatchNormalization object at 0x55a3f790>\n",
      "55: <keras.layers.advanced_activations.ReLU object at 0x55aa7b50>\n",
      "56: <keras.layers.convolutional.DepthwiseConv2D object at 0x55a6f430>\n",
      "57: <keras.layers.normalization.BatchNormalization object at 0x55a1de90>\n",
      "58: <keras.layers.advanced_activations.ReLU object at 0x55a1deb0>\n",
      "59: <keras.layers.convolutional.Conv2D object at 0x559cac70>\n",
      "60: <keras.layers.normalization.BatchNormalization object at 0x55994f30>\n",
      "61: <keras.layers.advanced_activations.ReLU object at 0x55994f10>\n",
      "62: <keras.layers.convolutional.DepthwiseConv2D object at 0x559420d0>\n",
      "63: <keras.layers.normalization.BatchNormalization object at 0x5596ff30>\n",
      "64: <keras.layers.advanced_activations.ReLU object at 0x5595ded0>\n",
      "65: <keras.layers.convolutional.Conv2D object at 0x559219d0>\n",
      "66: <keras.layers.normalization.BatchNormalization object at 0x558cda90>\n",
      "67: <keras.layers.advanced_activations.ReLU object at 0x558cde70>\n",
      "68: <keras.layers.convolutional.DepthwiseConv2D object at 0x55883e10>\n",
      "69: <keras.layers.normalization.BatchNormalization object at 0x558afcb0>\n",
      "70: <keras.layers.advanced_activations.ReLU object at 0x558affd0>\n",
      "71: <keras.layers.convolutional.Conv2D object at 0x558638d0>\n",
      "72: <keras.layers.normalization.BatchNormalization object at 0x5580ed50>\n",
      "73: <keras.layers.advanced_activations.ReLU object at 0x55876750>\n",
      "74: <keras.layers.convolutional.ZeroPadding2D object at 0x557bde30>\n",
      "75: <keras.layers.convolutional.DepthwiseConv2D object at 0x557f1db0>\n",
      "76: <keras.layers.normalization.BatchNormalization object at 0x557b4f50>\n",
      "77: <keras.layers.advanced_activations.ReLU object at 0x5579de10>\n",
      "78: <keras.layers.convolutional.Conv2D object at 0x5574fdd0>\n",
      "79: <keras.layers.normalization.BatchNormalization object at 0x55764a70>\n",
      "80: <keras.layers.advanced_activations.ReLU object at 0x55764950>\n",
      "81: <keras.layers.convolutional.DepthwiseConv2D object at 0x55717c30>\n",
      "82: <keras.layers.normalization.BatchNormalization object at 0x556c2ad0>\n",
      "83: <keras.layers.advanced_activations.ReLU object at 0x556c2290>\n",
      "84: <keras.layers.convolutional.Conv2D object at 0x556f6e70>\n",
      "85: <keras.layers.normalization.BatchNormalization object at 0x556a0ed0>\n",
      "86: <keras.layers.advanced_activations.ReLU object at 0x55688a50>\n",
      "87: <keras.layers.core.Flatten object at 0x55597e70>\n",
      "88: <keras.layers.core.Dense object at 0x54201310>\n",
      "89: <keras.layers.core.Activation object at 0x541edbd0>\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.0005\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "model = load_model()\n",
    "for i,layer in enumerate(model.layers):\n",
    "    print(\"{}: {}\".format(i,layer))\n",
    "# model.summary()\n",
    "\n",
    "# for layer in model.layers[:87]:\n",
    "#     layer.trainable=False\n",
    "# for layer in model.layers[87:]:\n",
    "#     layer.trainable=True\n",
    "opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay_rate, amsgrad=False)    \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 26s 755ms/step - loss: 6.5434 - accuracy: 0.6471\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 22s 653ms/step - loss: 7.9717e-05 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 22s 660ms/step - loss: 0.3928 - accuracy: 0.9118\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 22s 653ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 22s 651ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 22s 652ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 22s 650ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 22s 649ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 22s 660ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 22s 652ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x5c9851f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(TRAINING_DATA), np.array(TRAINING_LABELS), epochs=epochs, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.predict of <keras.engine.training.Model object at 0x5424f030>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('man-sleep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # with picamera.PiCamera() as camera:\n",
    "# #     camera.resolution = (1024, 768)\n",
    "# #     camera.framerate = 24    \n",
    "# #     with picamera.array.PiRGBArray(camera) as output:      \n",
    "# #         camera.capture(output, 'rgb')\n",
    "# #         print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "# #         frame = output.array\n",
    "# #         jpeg_image = bgr8_to_jpeg(frame)\n",
    "# #         image_widget.value = jpeg_image\n",
    "        \n",
    "# with picamera.PiCamera() as camera:\n",
    "#     camera.resolution = (480, 320)\n",
    "#     camera.framerate = 24        \n",
    "#     with picamera.array.PiRGBArray(camera) as output:\n",
    "#         while True:        \n",
    "#             camera.capture(output, 'rgb')\n",
    "#             print('Captured %dx%d image' % (output.array.shape[1], output.array.shape[0]))\n",
    "#             frame = output.array\n",
    "#             processed_image = prepare_frame(frame)  # prepare frame\n",
    "#             prediction = model.predict(processed_image)\n",
    "#             result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "#             print('predict class= %d' % result)\n",
    "#             output.truncate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "text = widgets.Text(\n",
    "    value='Hello World',\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "# model.save('my_model.h5')\n",
    "\n",
    "# # Deletes the existing model\n",
    "# del model  \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model('man-sleep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paho.mqtt.client.MQTTMessageInfo at 0x64f8e5a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "mqttc = mqtt.Client()\n",
    "mqttc.connect(\"mqtt.cmmc.io\", 1883, 60)\n",
    "\n",
    "mqttc.publish(\"CMMC/PLUG-001/$/command\", \"ON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "PiCameraMMALError",
     "evalue": "Failed to enable connection: Out of resources",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPiCameraMMALError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6728aeaef56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# initialize the camera and stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcamera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPiCamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# camera.resolution = (320, 240)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/picamera/camera.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, camera_num, stereo_mode, stereo_decimate, resolution, framerate, sensor_mode, led_pin, clock_mode, framerate_range)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstereo_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstereo_decimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensor_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframerate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclock_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_preview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_camera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/picamera/camera.py\u001b[0m in \u001b[0;36m_init_preview\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# fade to black (issue #22)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         self._preview = PiNullSink(\n\u001b[0;32m--> 513\u001b[0;31m             self, self._camera.outputs[self.CAMERA_PREVIEW_PORT])\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_start_capture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/picamera/renderers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, source)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMMALNullSink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/picamera/mmalobj.py\u001b[0m in \u001b[0;36menable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         mmal_check(\n\u001b[1;32m   2211\u001b[0m             \u001b[0mmmal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmal_connection_enable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m             prefix=\"Failed to enable connection\")\n\u001b[0m\u001b[1;32m   2213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0mMMALPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_all_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/picamera/exc.py\u001b[0m in \u001b[0;36mmmal_check\u001b[0;34m(status, prefix)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \"\"\"\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmmal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMMAL_SUCCESS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPiCameraMMALError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPiCameraMMALError\u001b[0m: Failed to enable connection: Out of resources"
     ]
    }
   ],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# from __future__ import print_function\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "from picamera.array import PiRGBArray\n",
    "from picamera import PiCamera\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "from imutils.video import FPS\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "#camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# initialize the camera and stream\n",
    "camera = PiCamera()\n",
    "# camera.resolution = (320, 240)\n",
    "camera.framerate = 32\n",
    "camera.start_preview()\n",
    "# rawCapture = PiRGBArray(camera, size=(320, 240))\n",
    "rawCapture = PiRGBArray(camera)\n",
    "stream = camera.capture_continuous(rawCapture, format=\"bgr\",\n",
    "use_video_port=True)\n",
    "\n",
    "image_widget = widgets.Image()\n",
    "display(text)\n",
    "display(image_widget)\n",
    "\n",
    "\n",
    "for (i, f) in enumerate(stream):\n",
    "    frame = f.array\n",
    "    image_widget.value =  bgr8_to_jpeg(frame)\n",
    "    processed_image = prepare_frame(frame)  # prepare frame\n",
    "    t1 = datetime.datetime.now()\n",
    "    prediction = model.predict(processed_image)\n",
    "    t2 = datetime.datetime.now()\n",
    "    result = np.argmax(prediction)  # imagenet_utils.decode_predictions(prediction)\n",
    "    text.value = \"class=\" + str(result)\n",
    "    if str(result) == \"0\":\n",
    "        mqttc.publish(\"CMMC/PLUG-001/$/command\", \"ON\")\n",
    "    else:\n",
    "        mqttc.publish(\"CMMC/PLUG-001/$/command\", \"OFF\")\n",
    "    rawCapture.truncate(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
